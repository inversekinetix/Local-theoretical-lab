<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=cJLRyNNdbfauG58nyHg_907PHvigsIdfeR0a_7INJ6xbV0WvE1cEyAoIq5yYZlSc);ol{margin:0;padding:0}table td,table th{padding:0}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify;height:11pt}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"EB Garamond";font-style:normal}.c5{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c4{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:justify}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c6{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c7{font-weight:700;font-family:"EB Garamond"}.c3{font-weight:400;font-family:"EB Garamond"}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c6 doc-content"><p class="c4 title" id="h.o81j3pr4u4et"><span class="c7">Commonsense reasoning</span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective.</span></p><p class="c1"><span class="c2">We know that infants and children arrive at an intuitive and seemingly flawless mathematical understanding of basic number patterns and structures (e.g., the existence of capable of counting to 10, but definitely not up to 100 because we know they can&#39;t) way before they learn the (&quot;adult&quot;) names for the natural numbers.</span></p><p class="c1"><span class="c2">Nevertheless, if you ask them why they know 10 is the right number, they can&#39;t really tell you because it&#39;s obvious in a kind of Stone Age way. They aren&#39;t capable of explaining what they&#39;re seeing directly. They understand but they can&#39;t step back and think.</span></p><p class="c1"><span class="c2">Similarly, there are judgements that you make daily that you can&#39;t explain to yourself. If you take a closer look you&#39;ll probably find, to your surprise, that they violate a key principle of logic.</span></p><p class="c1"><span class="c2">So it&#39;s not about whether the constructed is involved in the back. It&#39;s not about whether one commits common errors, such as double-counting bins coming to the wrong conclusions. Indeed one of the strengths of the recent advance in AI is the identification of the &quot;errors&quot; different people make when thinking about the problem, such as in opinions about the causes of obesity, bias, and climate change.</span></p><p class="c1"><span class="c2">This is the great goal that philosophers, psychologists and statisticians have been working on: to understand what fills our consciousness and we simply cannot explain in words what we so constantly experience. The real subject that lies at the heart of this research is direct experience, as opposed to what causes direct experience. The reductionists haven&#39;t been paying attention, because the only way to genuinely reduce is to push forward by questioning the established viewpoints of what&#39;s out there.</span></p><p class="c1"><span class="c2">That&#39;s what is happening to philosophy and the status quo more broadly in our time is what&#39;s leading to the development of the sort of AI we&#39;re rapidly developing.</span></p><p class="c1"><span class="c2">So it&#39;s not just about the technology.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Regarding your question:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">How does working in such encultured languages as Meecham and</span></p><p class="c1"><span class="c2">collaboratively with other software engineering team members address</span></p><p class="c1"><span class="c2">some of the decision making methodologies that would otherwise be</span></p><p class="c1"><span class="c2">difficult if not impossible?</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Haha. :-)</span></p><p class="c1"><span class="c2">Well, you&#39;re explicitly addressing whether encultured or the underlying languages are better. I&#39;ve been sitting on both sides recently. To me, choosing the right professional enculturation is much more democratirz (Take THAT, those who follow the traditional modes of training!) than what you may have seen in the movie. The tech demigod is nothing more than the rest of us with harmonic voices and competent choices at the levels of culture and pedagogy.</span></p><p class="c1"><span class="c2">What&#39;s just as amazing is that decisions have consequences: You can find tons of examples of that upcoming teams. Now, if you alter the structure of the team building and unit assignments, then chances are that you&#39;ll change the outcome. It&#39;s this: Unit (roberts, we can&#39;t stop the signal on &quot;some&quot;) of action that you&#39;re making.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">A:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">One can certainly train an AI model using commonsense reasoning. The key is to approximate the reality you want to model well enough so that the error between the human representation of the model and the actual reality of the model is not huge. Something like this:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">This last one is obviously an oversimplification, just to make an example. Using the Commonsense Reasoning from Nature&#39;s Algorithm is feasible under certain conditions. And it&#39;s problematic in a way because different cultural backgrounds will certainly lead to different models:</span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective because it assumes that things are similar to how the world is. </span></p><p class="c1"><span class="c2">For instance, the type of context reasoning we use when constructing a sentence; we want to associate the subject and verb with the object. </span></p><p class="c1"><span class="c2">AI uses a technique called &#39;weak supervision&#39; which is not realistic for this type of context reasoning, where the context (like the subject and verb) parts of the sentence have not been given at the same time as the entity that is being referred to in the subject and verb.</span></p><p class="c1"><span class="c2">AI does use it for other types of conceptual reasoning where the context has been provided. These types of reasoning can be quite powerful and useful.</span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective because it&#39;s entirely ambiguous what I mean by a &quot;commonsense relation&quot; in the first place, and because the meaning of &quot;a relation&quot; might not be even close to the same in the two expressions I use:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">When I say &quot;This relation/process is commutative&quot;</span></p><p class="c1"><span class="c2">When I say &quot;This conclusion of this application of this machine learning entity only makes sense if this is a valid application.&quot;</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Should they be according to you and should they be able to be fully trained, obviously a normal set of rules for commonsense reasoning should be able to be learnable and for them to have some meaning.</span></p><p class="c1"><span class="c2">But, here is a good question for you to investigate: How would you unify all of the human cultural and individual differences, the intentional modifications to the blueprints to something bought on a store shelf as (for instance) a &quot;Commercial 2MP Digital Camera from A to B&quot; with the commonsense reasoning more useful to know how the camera actually works, to make a rock solid predictor of the operation of any commercial digital camera apt to perform its specified functionality with the documented Total Track Losses over the Assumed Life Span of the Camera?</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">A:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Because computer systems don&#39;t have a common sense.</span></p><p class="c1"><span class="c2">They instead need to be trained by hand grants on a specific hardware and software setup. An example, if we could train a very inexpensive 4-legged robot in the manner of a cat, by reading about cats and then letting that information slowly drip in as the machine persisted over time learning about the environment, we would end up with a very expensive feline that wouldn&#39;t survive much longer than the dollar window in which it could be acquired.</span></p><p class="c1"><span class="c2">Basic.That is the key to making computers more human.The only limits of a &quot;computer&quot; are the concepts that make it different from a human. If a computer can learn to unerringly detect and track a stranger in a crowded room, the next step is to PEMDAS train it to understand the basic dangers and knowledge enough to make a policy decision that keeps others safe.If a computer recognizes a picture of a cat, the next step is to make a PICTURE GAME where moving a sheet of paper with your hand over the screen runs a cat.</span></p><p class="c1"><span class="c2">For neural networks it becomes: I created a program where I repeated the same sentence to the AI over and over within the first &quot;training phase&quot;.It needs words and their context before starting to link them up. After that it needs to have clear markers.Blue make you calm.Red makes you angry.</span></p><p class="c1"><span class="c2">Now computers need to learn the &quot;under currents&quot; in society.We&#39;re are starting to use a bar and a number below the bar to show how likely is any heading to be clicked on.Soon we&#39;ll be able to run a &quot;program&quot; that will not only rerun a document but also gather metrics on how frequently and immediately it was being read.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">A:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">For reference, rather than just tell you the answer, the following articles from Amy Zhang at OpenAI, two from Nature and another from CNRS/Universit&eacute; Paris-Saclay may be of interest on this topic:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Researchers Advance Machine Rule-Based Language Following Its Nose</span></p><p class="c1"><span class="c2">And Kissing Its Foot!</span></p><p class="c1"><span class="c2">Learning to classify and synthesize by reading and writing _{x,y}_ in</span></p><p class="c1"><span class="c2">the wild</span></p><p class="c1"><span class="c2">Generating Contextualized Sequences From End-to-End Generative Models</span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective for building an emulator because the model is more complex than the data allows. The model must be a holistic (top-down) approach rather one that learns from the bottom-up (feed-forward or supervised &quot;explicit&quot; layers). It has been proven, AI doesn&#39;t work. The AI bots get off track and get tricked all the time.</span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective at training an AI system for a few reasons.</span></p><p class="c1"><span class="c2">First, it is hard to define exactly what &quot;commonsense reasoning&quot; is meant to be capable of.The reason substantial research was done in this area and not all is summarized in the following quote:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Linguistic knowledge is exploited because the problem of context dependency is the most basic one in all of linguistic knowledge. It is important to recognize, however, that many current deep learning systems are not getting better at predicting perfectly what humans understand from a text, which suggests that our linguistic knowledge is more like a rough sketch, all the details of which have to be filled in by systems that use more sophisticated learning mechanisms [and optimisation by the model creator]. Increasingly, research in natural language processing is shifting from research from rule learning to instrumental models, which must be evaluated against surface-level corpus behaviours [...] </span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Second, mere exposure (being exposed to commonsense reasoning thinking patterns like commonsense reasoning, in an abstract sense) that an AI system accumulates mostly gets stored in its associative memories and not its schema/knowledge base. The schematized knowledge is the schema/knowledge base and not the associative memories in an AI system.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Many AI systems greatly benefit from the richness of associative memories. (Consider a human, for example.) But few AI systems are </span></p><p class="c1"><span class="c2">aimed at general-purpose commonsense reasoning&mdash;that is, few systems would be able to answer the kind of questions that require analysis at a very high level of abstraction.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">To generalize into a narrow domain (especially the domain of commonsense reasoning that we want of a language-trained AI system), it&#39;s good to understand what it is that works and doesn&#39;t work. (Academic research (and to a less degree, commercial applications) tries to evaluate the performance of human-like commonsense reasoning and these are not included in machine-learning and language-based AI systems outside of academic research.)</span></p><p class="c1"><span class="c2">All this being said, you can try out some machine-learned tools to unintendendedly complement it. The reason why Artificial Intelligence developed is partly to counter the absence of human beings which means most of the the data is trained using data that is created and operationalized by humans. Using such tools is not only to make tasks faster (bias the final solution) but also to make the system resilient to the limits of human capabilities (especially those of human beings that do not possess the AI systems capability of datamining).</span></p><p class="c0"><span class="c2"></span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">Commonsense reasoning is not effective for training an AI model because it requires knowledge that is either inaccessible (to the model), or more often, difficult to acquire in real time on the appropriate timescale.</span></p><p class="c1"><span class="c2">The &quot;vaguely human thing to do&quot; is to advise the candidate of course, telling her to take it slow, make sure that she can explain the item and has just enough metadata or context (and that there&#39;s not too much else to care about in the problem at hand), but doing it this way throws away much of the benefit of recommending the best course of action given the current state of the pipeline. Imagine if a computer asked you why you flipped a switch when you were designing a new circuit from scratch. This, right now, is a very hard problem to solve. The shift occurs because telling a human in slow-motion often makes no difference.</span></p><p class="c1"><span class="c2">For an example if this, imagine that a human was tasked o making sense of computer logs from telemetry data gathering equipment, and use that to diagnose a major feedback glitch in a piece of equipment. The human&#39;s knowledge base, and likely the exact data, is probably not stored or accessible off-hand. Or perhaps you have a space faring system that is flying a mission consisting of many, many items and sensors around the solar system and needs to determine if there is a broken part in a robotic housekeeping module or an inopportune sunlight reading is a problem with the CCD&#39;s or a problem with the onboard computer&#39;s health.</span></p><p class="c1"><span class="c2">The sessions are limited to 2-4 hours in length. That&#39;s long enough for her to represent the data and understand the problem at hand, but short enough that she doesn&#39;t get burnt out (except possibly with the pleasure of challenge).</span></p><p class="c1"><span class="c2">Note that there are well defined tasks that could be trivially trivialized using insight based-learning but I&#39;d need to know more about those requirements and the current state of the system for an appropriate solution. My opinion is that seeing a long term vision for this is more important than the implementation of a trivial version at the current state of the process.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">A:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">The task is essentially how to let the AI agent find effectively a function that employs as a generalisation as possible the knowledge it knows, and employ this function to understand and decide on the next move.</span></p><p class="c1"><span class="c2">We can classify this into two problems: Firstly, how can I find effective/correct program structures from unknown tasks; secondly, given a decision tree, and human coded program (or really any program, that we can directly inspect), tell if it was coded by a competent programmer.</span></p><p class="c1"><span class="c2">Intuitively, we can think that &quot;we&quot; want an algorithm (or program) that behaves as closely as possible to an analog thinker. This is what we get. The key idea is to use reinforcement learning. This, basically, assumes a model of the world. It will &quot;explore&quot; the possible program structures by itself (using random search, as a grab bag approach), and above a threshold in accuracy it will then &quot;converge&quot; towards a &quot;final answer&quot; (using for example a gradient descent type algorithm). </span></p><p class="c1"><span class="c2">Specifically, there is a generative model: this is a model of the world and all the structures (in our case, programs, but depending on the program it could also be &quot;alarm arrays&quot; or other hand drawing, or in the extreme, the human programmer&#39;s &quot;mind&quot;). This model is is used to build a program, and then it is used to decide on the next move of the play.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">A:</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">You can build machine learning models which do this. For example you could use a relational machine learner-&quot;now, how could I model this as a graph or point set...&quot;.</span></p><p class="c1"><span class="c2">http://scholar.harvard.edu/files/koutra/files/rl-jmlr.pdf</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">But, I would add that the practicalities are not so good. You can employ the modeling aspects of machine learning modeling in a machine learning workflow to support vision or reasoning more than you can in a conventional workflow. You get out of the traditional system what you put into it.</span></p><p class="c1"><span class="c2">But the other way around is less true. What you put into the system is much more critical. Human experience and systematic knowledge may make this a more useful project, but its application computationally is a big challenge.</span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c2">This is a bit of a chicken and egg question. You have to know something about the problem to understand the value of using machine learning models in the first place. </span></p><p class="c0"><span class="c2"></span></p><p class="c1"><span class="c3">Moreover, why is it that when the system is given the exact same data (let&#39;s just say you&#39;ve identified a nanometer shift in a pixel in an xray), but asked a slightly different question, the system still malfunctions and can&#39;t adapt but when you provide it identical data presented from different angles, it can shift its answer?</span></p></body></html>